import feedparser
import google.generativeai as genai
import requests
import time
from datetime import datetime, timedelta
from time import mktime
import os
from bs4 import BeautifulSoup

# Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„ÛŒØ¯Ù‡Ø§
TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
CHANNEL_ID = os.environ["CHANNEL_ID"]
GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]

# Ù…Ù†Ø¨Ø¹: ÙÙ‚Ø· Ø²ÙˆÙ…ÛŒØª
RSS_URLS = [
    "https://www.zoomit.ir/feed/",
]

# Ù…Ø¯Ù„ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ø¸Ø±ÙÛŒØª Ø¨Ø§Ù„Ø§ (1000 Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø± Ø±ÙˆØ²)
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash-lite')

HISTORY_FILE = "history.txt"

def load_history():
    if not os.path.exists(HISTORY_FILE): return []
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def save_to_history(link, title):
    try:
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            f.write(f"{link}|{title}\n")
        os.system(f'git config --global user.name "News Bot"')
        os.system(f'git config --global user.email "bot@noreply.github.com"')
        os.system(f'git add {HISTORY_FILE}')
        os.system('git commit -m "Update history"')
        os.system('git push')
    except: pass

def check_is_duplicate_topic(new_title, history_lines):
    recent_titles = [line.split("|")[1] for line in history_lines[-50:] if len(line.split("|")) > 1]
    if not recent_titles: return False
    
    prompt = f"""
    Ù„ÛŒØ³Øª ØªÛŒØªØ±Ù‡Ø§ÛŒ Ø§Ø®ÛŒØ±: {recent_titles}
    ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯: '{new_title}'
    Ø¢ÛŒØ§ Ø§ÛŒÙ† ØªÛŒØªØ± Ø¬Ø¯ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† Ø®Ø¨Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ù„ÛŒØ³Øª Ø¨Ø§Ù„Ø§ Ø¨ÙˆØ¯Ù‡ØŸ (Ø­ØªÛŒ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ù…ØªÙØ§ÙˆØª).
    ÙÙ‚Ø· Ø¨Ù†ÙˆÛŒØ³: YES ÛŒØ§ NO
    """
    try:
        return "YES" in model.generate_content(prompt).text.strip().upper()
    except: return False

def send_to_telegram(message, image_url=None):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/" + ("sendPhoto" if image_url else "sendMessage")
        data = {"chat_id": CHANNEL_ID, "parse_mode": "Markdown"}
        if image_url:
            data["photo"] = image_url
            data["caption"] = message
        else:
            data["text"] = message
        requests.post(url, data=data)
    except Exception as e: print(f"Send Error: {e}")

def extract_image(entry):
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ú©Ø³ Ù…Ø®ØµÙˆØµ Ø²ÙˆÙ…ÛŒØª
    try:
        if 'media_content' in entry: return entry.media_content[0]['url']
        if 'links' in entry:
            for l in entry.links:
                if l.type.startswith('image/'): return l.href
        if 'summary' in entry:
            soup = BeautifulSoup(entry.summary, 'html.parser')
            img = soup.find('img')
            if img: return img['src']
    except: pass
    return None

def summarize_with_ai(title, content):
    prompt = f"""
    ØªÙˆ Ø§Ø¯Ù…ÛŒÙ† Ú©Ø§Ù†Ø§Ù„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ù‡Ø³ØªÛŒ.
    Ø®Ø¨Ø±: {title}
    Ù…ØªÙ†: {content}

    ÙˆØ¸Ø§ÛŒÙ:
    1. ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ Ø¬Ø°Ø§Ø¨ Ùˆ Ú©ÙˆØªØ§Ù‡ (Ø­Ø¯ÙˆØ¯ 3 Ø®Ø·) Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³.
    2. Ù„ÛŒÙ†Ú© Ù…Ù†Ø¨Ø¹ Ù†Ú¯Ø°Ø§Ø±.
    3. Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
    4. Ø¯Ø± Ø¢Ø®Ø± Ø¨Ù†ÙˆÛŒØ³: ðŸ†” @Teklp
    """
    try: return model.generate_content(prompt).text
    except: return None

def check_feeds():
    history_lines = load_history()
    history_links = [line.split("|")[0] for line in history_lines]
    
    # Ø¨Ø±Ø±Ø³ÛŒ 40 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø§Ø®ÛŒØ± (Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ 30 Ø¯Ù‚ÛŒÙ‚Ù‡â€ŒØ§ÛŒ)
    time_threshold = datetime.now() - timedelta(minutes=40)
    
    for url in RSS_URLS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if hasattr(entry, 'published_parsed'):
                    pub_date = datetime.fromtimestamp(mktime(entry.published_parsed))
                    
                    if pub_date > time_threshold:
                        if entry.link in history_links: continue
                        if check_is_duplicate_topic(entry.title, history_lines):
                            save_to_history(entry.link, entry.title)
                            continue
                        
                        summary = summarize_with_ai(entry.title, entry.summary)
                        if summary:
                            final_text = f"ðŸ”¥ **{entry.title}**\n\n{summary}"
                            send_to_telegram(final_text, extract_image(entry))
                            save_to_history(entry.link, entry.title)
                            time.sleep(5)
        except Exception as e: print(f"Feed Error: {e}")

if __name__ == "__main__":
    check_feeds()
